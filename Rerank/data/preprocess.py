import json
import os
from pathlib import Path
from dataclasses import dataclass
from typing import Dict, List, Any
import logging

# é…ç½®æ—¥å¿—æ ¼å¼
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler("preprocess.log"),
        logging.StreamHandler()
    ]
)

@dataclass
class LampSample:
    """ç»Ÿä¸€æ•°æ®ç»“æ„ç±»"""
    task: str
    sample_id: str
    input_text: str
    output: str
    profile: List[Dict[str, str]]
    split_type: str  # 'user' or 'time'

class LampPreprocessor:
    def __init__(self, raw_dir: str = "data/raw", output_dir: str = "data/processed"):
        self.raw_dir = Path(raw_dir)
        self.output_dir = Path(output_dir)
        self._create_dirs()
        
        # ä»»åŠ¡å¤„ç†æ˜ å°„è¡¨
        self.task_processors = {
            "LaMP_1": self._process_lamp1,
            "LaMP_2": self._process_lamp2,
            #"LaMP_3": self._process_lamp3,
            #"LaMP_4": self._process_lamp4,
            #"LaMP_5": self._process_lamp5,
            #"LaMP_6": self._process_lamp6,
            #"LaMP_7": self._process_lamp7,
        }

    def _create_dirs(self):
        """åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„"""
        (self.output_dir / "user_split").mkdir(parents=True, exist_ok=True)
        (self.output_dir / "time_split").mkdir(parents=True, exist_ok=True)

    def process_all(self):
        """å¤„ç†æ‰€æœ‰æ£€æµ‹åˆ°çš„ä»»åŠ¡"""
        for task_dir in self.raw_dir.glob("LaMP_*"):
            task_name = task_dir.name
            if task_name in self.task_processors:
                logging.info(f"ğŸ”¨ å¼€å§‹å¤„ç†ä»»åŠ¡: {task_name}")
                self.task_processors[task_name](task_dir)
                logging.info(f"âœ… å®Œæˆå¤„ç†ä»»åŠ¡: {task_name}")
            else:
                logging.warning(f"âš ï¸ å‘ç°æœªå®šä¹‰ä»»åŠ¡ç›®å½•: {task_name}")

    def _load_task_data(self, task_dir: Path):
        """åŠ è½½ä»»åŠ¡æ•°æ®"""
        with open(task_dir / "train_questions.json") as f:
            questions = json.load(f)
        
        with open(task_dir / "train_outputs.json") as f:
            outputs = json.load(f)
        
        return questions, outputs

    def _process_lamp1(self, task_dir: Path):
        """å¤„ç†å­¦æœ¯å¼•ç”¨è¯†åˆ«ä»»åŠ¡"""
        questions, outputs = self._load_task_data(task_dir)
        output_map = {o["id"]: o["output"] for o in outputs["golds"]}
        
        samples = []
        for q in questions:
            sample = LampSample(
                task="LaMP_1",
                sample_id=q["id"],
                input_text=self._clean_input(q["input"], prefix="For an author who has written"),
                output=self._parse_output(output_map[q["id"]], task=1),
                profile=[self._format_profile(p, fields=["title", "abstract"]) for p in q["profile"]],
                split_type=self._detect_split_type(q["id"])
            )
            samples.append(sample)
        
        self._save_samples(samples, task_dir.name)

    def _process_lamp2(self, task_dir: Path):
        """å¤„ç†ç”µå½±åˆ†ç±»ä»»åŠ¡"""
        questions, outputs = self._load_task_data(task_dir)
        output_map = {o["id"]: o["output"] for o in outputs["golds"]}
        
        samples = []
        for q in questions:
            sample = LampSample(
                task="LaMP_2",
                sample_id=q["id"],
                input_text=self._clean_input(q["input"], prefix="Which category"),
                output=output_map[q["id"]].lower(),
                profile=[self._format_profile(p, fields=["text", "category", "title"]) for p in q["profile"]],
                split_type=self._detect_split_type(q["id"])
            )
            samples.append(sample)
        
        self._save_samples(samples, task_dir.name)

    # å…¶ä»–ä»»åŠ¡å¤„ç†å‡½æ•°ç»“æ„ç±»ä¼¼ï¼Œæ ¹æ®å…·ä½“æ•°æ®ç»“æ„è°ƒæ•´

    def _clean_input(self, text: str, prefix: str = None) -> str:
        """æ¸…æ´—è¾“å…¥æ–‡æœ¬"""
        if prefix and text.startswith(prefix):
            return text[len(prefix):].strip().capitalize()
        return text.strip()

    def _parse_output(self, output: str, task: int) -> str:
        """è§£æä¸åŒä»»åŠ¡çš„è¾“å‡ºæ ¼å¼"""
        if task == 1:
            return output.strip("[]")
        return output.strip()

    def _format_profile(self, profile_item: Dict, fields: List[str]) -> Dict:
        """æ ¼å¼åŒ–ç”¨æˆ·ç”»åƒæ¡ç›®"""
        return {field: profile_item.get(field, "")[:1000] for field in fields}  # é™åˆ¶é•¿åº¦

    def _detect_split_type(self, sample_id: str) -> str:
        """æ ¹æ®æ ·æœ¬IDæ£€æµ‹åˆ†å‰²ç±»å‹ï¼ˆç¤ºä¾‹é€»è¾‘ï¼‰"""
        return "user" if int(sample_id) % 2 == 0 else "time"

    def _save_samples(self, samples: List[LampSample], task_name: str):
        """ä¿å­˜å¤„ç†åçš„æ ·æœ¬"""
        split_types = {}
        for sample in samples:
            split_types.setdefault(sample.split_type, []).append(sample)
        
        for split_type, split_samples in split_types.items():
            output_path = self.output_dir / f"{split_type}_split" / f"{task_name}.jsonl"
            
            with open(output_path, "w") as f:
                for sample in split_samples:
                    f.write(json.dumps({
                        "task": sample.task,
                        "id": sample.sample_id,
                        "input": sample.input_text,
                        "output": sample.output,
                        "profile": sample.profile
                    }) + "\n")
            
            logging.info(f"ğŸ’¾ ä¿å­˜ {len(split_samples)} ä¸ªæ ·æœ¬åˆ° {output_path}")

if __name__ == "__main__":
    processor = LampPreprocessor()
    try:
        processor.process_all()
        logging.info("ğŸ‰ æ‰€æœ‰æ•°æ®å¤„ç†å®Œæˆï¼")
    except Exception as e:
        logging.error(f"âŒ å¤„ç†å¤±è´¥: {str(e)}")
        raise