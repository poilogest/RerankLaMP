import torch
import torch.nn as nn
import argparse
from retriever import deep_cluster, k_means_cluster, bm25_select_profiles
import json
import yaml
from model import BertForClassification
from transformers import BertTokenizer, AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments
from copy import deepcopy
from retriever import build_corpus_and_query
from prompts.prompts import simple_prompt_generator
from datasets import Dataset  # å¯¼å…¥HF Dataset
from tqdm.auto import tqdm
import os

def generate_data(model, tokenizer, source_data, cluster_data, target_data, task, task_config, k):
    # 1. é¢„å¤„ç†ä¼˜åŒ–
    max_length = 1024
    device = model.device
    prompt_generator = simple_prompt_generator()
    
    # 2. æ‰¹é‡æ£€ç´¢ï¼ˆå‡è®¾bm25_select_profilesæ”¯æŒæ‰¹é‡ï¼‰
    retrieval_data = bm25_select_profiles(cluster_data, task, task_config, k)
    
    # 3. é¢„ç”Ÿæˆæ‰€æœ‰promptå˜ä½“ï¼ˆæ·»åŠ è¿›åº¦æ¡ï¼‰
    batch_inputs = []
    batch_targets = []
    inputs = []
    
    # è®¡ç®—æ€»è¿›åº¦æ­¥æ•°
    total_profiles = sum(len(data['profile']) for data in source_data)
    total_combinations = total_profiles * k
    
    with tqdm(total=total_combinations, 
             desc="ğŸ”„ Generating prompt variants",
             unit="combo") as pbar:
        
        for data_idx, data in enumerate(source_data):
            profiles = data['profile']
            current_retrieval = retrieval_data[data_idx]
            
            # é¢„ç”ŸæˆåŸºç¡€è¯­æ–™
            corpus, _, _ = build_corpus_and_query(task, data['input'], profiles, task_config)
            
            # ç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„profileæ›¿æ¢ç»„åˆ
            for profile_idx, profile in enumerate(profiles):
                inputs.append(corpus[profile_idx])
                for pos in range(k):
                    modified_retrieval = current_retrieval.copy()
                    modified_retrieval["profile"][pos] = profile
                    prompt = prompt_generator(data['input'], modified_retrieval["profile"], task)
                    target_output = target_data[data_idx]["output"]
                    
                    # ============== å…³é”®ä¿®æ”¹å¼€å§‹ ==============
                    # åˆ†åˆ«ç¼–ç promptå’Œtargetï¼ˆä¸æ·»åŠ ç‰¹æ®Štokenï¼‰
                    prompt_tokens = tokenizer.encode(prompt, add_special_tokens=False)
                    target_tokens = tokenizer.encode(target_output, add_special_tokens=False)
                    
                    # åˆå¹¶ä¸ºå®Œæ•´tokenåºåˆ—å¹¶è§£ç 
                    full_tokens = prompt_tokens + target_tokens
                    full_text = tokenizer.decode(full_tokens, skip_special_tokens=True)
                    
                    batch_inputs.append(full_text)
                    batch_targets.append(len(prompt_tokens))  # å­˜å‚¨tokençº§é•¿åº¦
                    
                    pbar.update(1)
                    pbar.set_postfix({
                        'data_idx': data_idx,
                        'current_profile': f"{profile_idx+1}/{len(profiles)}",
                        'current_pos': pos+1
                    })

    log_probs = []
    
    # è®¾ç½®å­æ‰¹æ¬¡å¤§å°ä»¥å‡å°‘æ˜¾å­˜æ¶ˆè€—
    sub_batch_size = 4 # æ ¹æ®æ˜¾å­˜æƒ…å†µè°ƒæ•´è¯¥å€¼
    
    # ä¸»è¿›åº¦æ¡ï¼ˆåŒ…å«å­æ‰¹æ¬¡å¤„ç†ï¼‰
    with tqdm(total=len(batch_inputs), desc="ğŸ” Processing sub-batches", 
             unit="sample", postfix={"sub_batch": 0}) as main_pbar:
        
        # åˆ†æ‰¹æ¬¡å¤„ç†æ•°æ®
        for sub_idx in range(0, len(batch_inputs), sub_batch_size):
            sub_inputs = batch_inputs[sub_idx:sub_idx + sub_batch_size]
            sub_targets = batch_targets[sub_idx:sub_idx + sub_batch_size]
            
            # æ›´æ–°ä¸»è¿›åº¦æ¡çŠ¶æ€
            main_pbar.set_postfix({"sub_batch": sub_idx//sub_batch_size + 1})
            

            
    
            # 4. å­æ‰¹æ¬¡ç¼–ç ï¼ˆæ— å†…éƒ¨è¿›åº¦æ¡ï¼‰
            encoded_sub = tokenizer(
                sub_inputs,
                padding=True,
                truncation=True,
                max_length=max_length,
                return_tensors="pt",
                add_special_tokens=True
            ).to(device)

            
            # 5. å­æ‰¹æ¬¡å‰å‘ä¼ æ’­ï¼ˆæ— å†…éƒ¨è¿›åº¦æ¡ï¼‰
            with torch.no_grad():
                outputs = model(**encoded_sub, labels=encoded_sub.input_ids)
                
                    
            # 6. è®¡ç®—å­æ‰¹æ¬¡log_probsï¼ˆæ— å†…éƒ¨è¿›åº¦æ¡ï¼‰
            for idx in range(len(sub_inputs)):
                prompt_len = sub_targets[idx]
                target_ids = encoded_sub.input_ids[idx, prompt_len:]
                logits = outputs.logits[idx, prompt_len-1:-1, :]
                log_prob = torch.log_softmax(logits, dim=-1)
                token_log_probs = log_prob.gather(-1, target_ids.unsqueeze(-1)).squeeze(-1)
                log_probs.append(token_log_probs.sum().item())
                
            
            # æ›´æ–°ä¸»è¿›åº¦æ¡
            main_pbar.update(len(sub_inputs))
    
    # 7. é‡ç»„ç»“æœå¹¶ç”Ÿæˆåˆ†ç±»æ ‡ç­¾
    labels = []
    ptr = 0
    
    # ç»“æœåŒ…è£…è¿›åº¦æ¡
    with tqdm(total=len(source_data), desc="ğŸ“¦ Packaging results", unit="data") as pkg_pbar:
        for data in source_data:
            for profile_idx, _ in enumerate(data['profile']):
                current_scores = log_probs[ptr:ptr+k]
                label_idx = current_scores.index(max(current_scores))
                labels.append(label_idx)
                ptr += k
            pkg_pbar.update(1)
            pkg_pbar.set_postfix({"profiles_packed": len(data['profile'])})
    
    return inputs, labels

parser = argparse.ArgumentParser()
parser.add_argument("--task", required=True)
parser.add_argument("--cache_dir", default = "./cache")


if __name__ == "__main__":
    os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
    opts = parser.parse_args()
    task = opts.task
    num_iterations = 3  # è·å–è¿­ä»£æ¬¡æ•°

    # åŠ è½½æ•°æ®
    source_data_addr = f"data/{task}/train_questions.json"
    target_data_addr = f"data/{task}/train_outputs.json"
    with open(source_data_addr) as file:
        source_data = json.load(file)
    with open(target_data_addr) as file:
        target_data = json.load(file)["golds"]
    source_data = source_data[0:100]
    target_data = target_data[0:100]
    # åŠ è½½é…ç½®
    config_path = "configs/base.yaml"
    config = yaml.safe_load(open(config_path, 'r'))
    task_config = config["task_configurations"].get(task)
    k = 2  # èšç±»æ•°é‡

    # åˆå§‹åŒ–æ¨¡å‹
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    classification_model_name = "bert-base-cased"
    classification_model = BertForClassification(classification_model_name, num_labels=k).to(device)
    classification_tokenizer = BertTokenizer.from_pretrained(classification_model_name, cache_dir=opts.cache_dir)

    generation_model_name = "Qwen/Qwen1.5-0.5B"
    generation_model = AutoModelForCausalLM.from_pretrained(generation_model_name, cache_dir=opts.cache_dir).to(device)
    generation_tokenizer = AutoTokenizer.from_pretrained(generation_model_name, cache_dir=opts.cache_dir)
    #generation_tokenizer.pad_token = generation_tokenizer.eos_token
    # è¿­ä»£è®­ç»ƒå¾ªç¯
    print(f"Tokenizerå®é™…max_length: {generation_tokenizer.model_max_length}")
    for iteration in range(num_iterations):
        print(f"\n=== è®­ç»ƒè¿­ä»£ {iteration + 1}/{num_iterations} ===")
        
        # 1. ä½¿ç”¨å½“å‰æ¨¡å‹èšç±»
        print("ğŸ”„ æ‰§è¡Œæ·±åº¦èšç±»...")
        cluster_data = deep_cluster(
            source_data, 
            task, 
            task_config, 
            classification_model, 
            classification_tokenizer, 
            k
        )
        
        # 2. ç”Ÿæˆè®­ç»ƒæ•°æ®
        print("âš™ï¸ ç”Ÿæˆå¢å¼ºæ•°æ®...")
        inputs, labels = generate_data(
            generation_model,
            generation_tokenizer,
            source_data,
            cluster_data,
            target_data,
            task,
            task_config,
            k
        )
        
        from collections import Counter

        # ç»Ÿè®¡æ ‡ç­¾åˆ†å¸ƒ
        label_counts = Counter(labels)
        print("\nğŸ“Š æ ‡ç­¾åˆ†å¸ƒç»Ÿè®¡:")
        for label, count in sorted(label_counts.items()):
            print(f"  ç±»åˆ« {label}: {count} æ ·æœ¬ ({count/len(labels):.1%})")

        # 3. å‡†å¤‡æ•°æ®é›†
        print("ğŸ“¦ å‡†å¤‡è®­ç»ƒæ•°æ®é›†...")
        def tokenize_function(examples):
            return classification_tokenizer(
                examples["text"],
                padding="max_length",
                truncation=True,
                max_length=512,
                return_tensors=None
            )
        
        hf_dataset = Dataset.from_dict({"text": inputs, "labels": labels})
        tokenized_dataset = hf_dataset.map(
            tokenize_function,
            batched=True,
            remove_columns=["text"]
        )
        tokenized_dataset.set_format(
            type="torch", 
            columns=["input_ids", "attention_mask", "labels"],
            output_all_columns=False
        )
        
        # 4. è®­ç»ƒåˆ†ç±»æ¨¡å‹
        print("ğŸ¯ è®­ç»ƒåˆ†ç±»æ¨¡å‹...")
        training_args = TrainingArguments(
            output_dir=f"output/iter_{iteration}",      # æ¯æ¬¡è¿­ä»£ä¿å­˜åˆ°ä¸åŒç›®å½•
            num_train_epochs=1,
            per_device_train_batch_size=4,
            logging_dir=f"logs/iter_{iteration}",
            logging_steps=50,
            save_strategy="no",
            learning_rate=5e-5,
            evaluation_strategy="no",
            report_to="none",
            weight_decay=0.01,
        )
        
        trainer = Trainer(
            model=classification_model,
            args=training_args,
            train_dataset=tokenized_dataset
        )
        trainer.train()
        model_save_path = os.path.join(training_args.output_dir, "full_model.pth")
        torch.save(classification_model, model_save_path)