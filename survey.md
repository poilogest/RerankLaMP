# 个性化大语言模型

---

## 论文1：基于七层个性化金字塔的神经符号情感分析方法  
**年份**：2024 
**刊物**：HCII

### 研究问题  
传统情感分析忽视个体差异（性格/文化/职业），导致个性化场景效果受限  

### 核心方法  
- 提出个性化金字塔七层框架（Entity, Culture, Religion, Vocation, Ideology, Personality, and Subjectivity.）  


### 实验数据  
| 数据集       | 评估指标          | 关键结果                     |
|--------------|-------------------|------------------------------|
| Harry Potter对话集 | 准确率/F1/MSE | GPT-4准确率↑12.77%    (0.7321 -> 0.8598)        |

### 结论
- 所有七层均对结果有增益，文化层贡献最大

---

## 论文2：基于变分偏好学习的个性化人类反馈强化学习  
**年份**：2024  
**刊物**：NIPS

### 研究问题  
传统RLHF方法假设用户共享单一效用函数，无法处理多样性偏好（如对回答长度/风格的差异化需求），导致少数群体偏好被忽略  

### 核心方法  
- ​**变分偏好学习(VPL)**：使用贝叶斯变分推断通过潜在变量$z$建模未观测的用户偏好  
- ​**奖励缩放技术(SPO)**：解决不同用户奖励尺度不一致问题  

### 实验数据  
| 数据集       | 评估指标          |
|--------------|-------------------|
| UltraFeedback-P和合成数据集Pets |  accuracy of different reward models  |

### 结论  
1. VPL在偏好多样性场景下显著优于标准RLHF  


## 论文3：基于检索增强的LLM个性化生成优化方法  
**年份**：2024  
**刊物**：SIGIR

### 研究问题  
传统LLM无法有效利用用户背景数据实现个性化生成，直接微调模型导致高计算成本与隐私风险  

### 核心方法  
- ​**RAG**：  
  ▸ ROPG-RL：通过强化学习（生成质量反馈）优化检索模型  
  ▸ ROPG-KD：基于知识蒸馏对齐检索与生成分布  
- ​**动态检索器选择**：  
  ▸ RSPG-Pre/Post：生成前/后动态选择最优检索策略（BM25/语义检索等）  

### 实验数据  
| 数据集       | 评估指标          | 
|--------------|-------------------|
| LaMP基准 | 准确率/F1/MAE/ROUGE | 

### 结论  
1. ROPG-RL与ROPG-KD在6/7任务中超越基线  
2. RSPG-Post在所有任务超过基线
3. 不修改LLM参数实现隐私安全个性化生成